{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6617a09f",
   "metadata": {},
   "source": [
    "# Paper Summary\n",
    "\n",
    "The paper demonstrated that SOTA results could be obtained for downstream tasks such as textual entailment, question answering, semantic similarity assessment, and\n",
    "document classification by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning (with task-specific input transformations) on each specific task.\n",
    "\n",
    "Stage 1 - Unsupervised pre-training of a language model (transformer decoder) on the bookcorpus dataset\n",
    "Stage 2 - Supervised fine-tuning of model from stage 1 (with an added linear layer at the end) on a target task e.g. entailment using a labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdb0e2",
   "metadata": {},
   "source": [
    "In this setup, I train the exact `124M` param model on the bookcorpus dataset, evaluating the performance using perplexity where the original paper achieved perplexity of 18.4. Then I finetune on the Story cloze test dataset of textual entrailment task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840d330",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
